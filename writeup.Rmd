---
title: "writeup"
output: html_document
---

The first step is to declare our libraries and import the data.
Note we will only be importing the training set at this time.


```{r}
setwd("practicalmachinelearning/")
data<-read.csv("pml-training.csv",header=TRUE)
remColumns<-grep("NA",data[1,])
data<-data[,-remColumns]
data<-data[,(data[1,]!="")]
data<-data[!names(data) %in% c("X","user_name")]

#see if linearcombos for any of the numeric data
comboInfo <- findLinearCombos(data[,c(1:2,5:57)])
#we get no linear combos

#lets make sure we dont have any near zero variables
nzv<-nearZeroVar(data)
# we get one. we can see that .102 percent of the variables in new_window are unique
# thats very low, so we can remove the variable with the following
data$new_window<-NULL

#find correlations between all numeric variables. we will go high so set it to 99 percent
dCor <-  cor(data[,c(1,2,4:56)])
findCorrelation(dCor, cutoff = .99)

#we see its column 13, the accel_belt_y. to remove it
data$accel_belt_y<-NULL

library(caret)
inTrain<-createDataPartition(y=data$classe,p=.75,list=FALSE)
training<-data[inTrain,]
testing<-data[-inTrain,]

#particularly for ramdomForest, im setting it to use 3 
#of my processors when fitting the models
library(doMC)
registerDoMC(cores = 3)

set.seed(4582)
modelLinear<-train(classe~.,data=training,method="lda",trControl=trainControl(method="cv"))
#mtry is the number of randomly selected predictors
modelRF<-train(classe~.,data=training,method="rf",trControl=trainControl(method="cv"),tuneGrid=expand.grid(mtry=6))

print(modelRF$finalModel)

Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 6

        OOB estimate of  error rate: 0.12%
Confusion matrix:
     A    B    C    D    E  class.error
A 4184    1    0    0    0 0.0002389486
B    3 2845    0    0    0 0.0010533708
C    0    2 2562    3    0 0.0019477990
D    0    0    5 2406    1 0.0024875622
E    0    0    0    2 2704 0.0007390983


#may need to not use nmtry but use n.trees for number of boosting iterations
modelBoost<-train(classe~.,data=training,method="gbm",verbose=FALSE,trControl=trainControl(method="cv"))

head(summary(modelBoost),10)
                                                          var   rel.inf
raw_timestamp_part_1                     raw_timestamp_part_1 24.799770
roll_belt                                           roll_belt 14.384293
pitch_forearm                                   pitch_forearm  7.236918
num_window                                         num_window  7.188353
magnet_dumbbell_z                           magnet_dumbbell_z  4.826559
roll_forearm                                     roll_forearm  3.567871
cvtd_timestamp28/11/2011 14:15 cvtd_timestamp28/11/2011 14:15  3.202645
cvtd_timestamp30/11/2011 17:12 cvtd_timestamp30/11/2011 17:12  3.101513
pitch_belt                                         pitch_belt  2.493709
magnet_dumbbell_y                           magnet_dumbbell_y  2.443469


accLinear<-sum(predict(modelLinear,testing)==testing$classe)/length(testing$classe)
#comes out at 85.7 percent
accRF<-sum(predict(modelRF,testing)==testing$classe)/length(testing$classe)
#comes out at 99.90 percent accurate
accBoost<-sum(predict(modelBoost,testing)==testing$classe)/length(testing$classe)
#comes out at 99.67 percent accuracy. So out of sample error is .27 percent

testData<-read.csv("pml-testing.csv",header=TRUE)
submit<-predict(modelRF,testData)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(submit)

#They were all correct

```
Note that we get a table with 160 variables. classe is our Y variable, and X is just index.
A lot of the columns have missing data in them. Im going to get rid of any columns with NA.

remColumns<-grep("NA",data[1,])
data2<-data[,-remColumns]

Ill also get rid of any columns where the data is blank
data2<-data2[,(data2[1,]!="")]

at this point it has problem with the new_window variable. ill remove it with

data3<-data2
data3$new_window<-NULL

now its exactly the same as the other problem.

At this point we have 60 variables, including our Y variable, and index variable.
Im going to get rid of the index and the username variables, since those might skew our predictions.
We dont want to have a model that says "If its carlitos, then he is not doing the exercise right"
We want to look at the individual exercise and know if its bad or good based on the data.

data4<-data3[!names(data3) %in% c("X","user_name")]

Now we have 58 features, and 19622 rows where each row is an independent example.
We will now take the data and split it into testing and training. 

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
